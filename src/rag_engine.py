"""
RAG Response Generator with FAISS Vector Store + Gemma2-9B Q4
Integrates vector similarity search with LLM generation for intelligent responses
"""

import logging
from pathlib import Path
from typing import Optional, List, Dict
import sys

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from vector_store import VectorStoreManager
from gemma2_llm import Gemma2LLM

logger = logging.getLogger(__name__)


class RAGResponseGenerator:
    """Generate responses using RAG (Retrieval-Augmented Generation) with FAISS + Gemma2"""
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        vector_store_path: str = "faiss_index",
        data_dir: str = "data",
        use_llm: bool = True
    ):
        """
        Initialize RAG system
        
        Args:
            model_path: Path to Gemma2-9B Q4 model file (optional, will search automatically)
            vector_store_path: Path to FAISS index directory
            data_dir: Path to JSON data files
            use_llm: Whether to use Gemma2 LLM (if False, uses template responses)
        """
        self.use_llm = use_llm
        self.llm_available = False
        
        # Initialize vector store
        logger.info("ðŸ”„ Initializing RAG system...")
        
        self.vector_store = VectorStoreManager(
            data_dir=data_dir,
            vector_store_path=vector_store_path
        )
        
        # Load existing vector store
        if not self.vector_store.load_vector_store():
            logger.warning("âš ï¸ Vector store not found. Please run build_vector_store.py first.")
            logger.info("   Run: python build_vector_store.py")
        
        # Initialize LLM (if requested)
        self.llm = None
        if use_llm:
            try:
                logger.info("ðŸ¤– Initializing Gemma2-9B Q4 LLM...")
                self.llm = Gemma2LLM(
                    model_path=model_path,
                    n_ctx=4096,
                    n_gpu_layers=0,  # CPU only (set higher for GPU)
                    n_threads=4,
                    temperature=0.7,
                    max_tokens=512,
                    verbose=False
                )
                self.llm_available = self.llm.is_initialized
                
                if self.llm_available:
                    logger.info("âœ… RAG system ready (Vector Store + Gemma2 LLM)")
                else:
                    logger.warning("âš ï¸ Gemma2 not initialized. Using template responses.")
                    
            except Exception as e:
                logger.error(f"âŒ Failed to initialize LLM: {e}")
                logger.info("   Falling back to template-based responses")
                self.llm_available = False
        else:
            logger.info("âœ… RAG system ready (Vector Store only - template mode)")
    
    def generate_response(self, query: str, language: str = 'en', top_k: int = 3) -> str:
        """
        Generate response using RAG pipeline
        
        Args:
            query: User query
            language: Response language (en, ta, te, hi, kn, ml)
            top_k: Number of relevant chunks to retrieve
            
        Returns:
            Generated response
        """
        logger.info(f"ðŸŽ¯ Query: {query[:50]}... (Language: {language})")
        
        # Check for greeting queries
        if self._is_greeting(query):
            return self._get_greeting_response(language)
        
        # Step 1: Retrieve relevant context from vector store
        context_chunks = self.vector_store.search(query, top_k=top_k)
        
        if not context_chunks:
            logger.warning("âš ï¸ No relevant information found")
            return self._get_no_info_response(query, language)
        
        # Step 2: Format context
        context_text = self._format_context(context_chunks)
        logger.info(f"ðŸ“š Retrieved {len(context_chunks)} relevant chunks")
        
        # Step 3: Generate response
        if self.llm_available and self.llm:
            # Use Gemma2 LLM for generation
            response = self._generate_with_llm(query, context_text, language)
        else:
            # Use template-based response
            response = self._generate_template_response(query, context_chunks, language)
        
        return response
    
    def _format_context(self, chunks: List[Dict]) -> str:
        """Format retrieved chunks into context string"""
        context_parts = []
        
        for idx, chunk in enumerate(chunks, 1):
            source = chunk.get('source_file', 'unknown')
            text = chunk.get('text', '')
            score = chunk.get('similarity_score', 0)
            
            context_parts.append(f"[Source {idx}: {source} (relevance: {score:.2f})]\n{text}\n")
        
        return "\n".join(context_parts)
    
    def _generate_with_llm(self, query: str, context: str, language: str) -> str:
        """Generate response using Gemma2 LLM"""
        try:
            logger.info("ðŸ¤– Generating response with Gemma2...")
            
            # Add language instruction if not English
            lang_instruction = ""
            if language != 'en':
                lang_map = {
                    'ta': 'Tamil',
                    'te': 'Telugu',
                    'hi': 'Hindi',
                    'kn': 'Kannada',
                    'ml': 'Malayalam'
                }
                lang_name = lang_map.get(language, 'English')
                lang_instruction = f"\nIMPORTANT: Respond in {lang_name} language."
            
            response = self.llm.generate(
                prompt=query + lang_instruction,
                context=context,
                max_tokens=512,
                temperature=0.7
            )
            
            logger.info(f"âœ… Generated {len(response)} chars")
            return response
            
        except Exception as e:
            logger.error(f"âŒ LLM generation error: {e}")
            return self._generate_template_response(query, [], language)
    
    def _generate_template_response(self, query: str, chunks: List[Dict], language: str) -> str:
        """Generate template-based response (fallback)"""
        logger.info("ðŸ“ Using template response")
        
        if not chunks:
            return self._get_no_info_response(query, language)
        
        # Use smart extraction instead of raw dump
        response = self._format_natural_response(query, chunks)
        
        return response
    
    def _format_natural_response(self, query: str, chunks: List[Dict]) -> str:
        """Format context into natural, conversational response using LLM-style generation"""
        import re
        
        query_lower = query.lower()
        
        # Combine all chunk text
        full_context = "\n".join([chunk.get('text', '') for chunk in chunks])
        
        # Use a simple LLM-like prompt system to generate natural responses
        system_prompt = """You are KARE AI Assistant. Given the context from university knowledge base, answer the user's question in a clear, natural, and conversational way. 

Extract relevant information from the context and present it in a well-organized format.
For fees, show amounts clearly with currency symbols.
Be concise but complete.
Do not repeat metadata or technical formatting."""

        # Parse the context to extract structured information
        context_data = self._parse_markdown_context(full_context)
        
        # Generate response based on query type
        response = self._generate_llm_style_response(query, query_lower, context_data, full_context)
        
        return response
    
    def _parse_markdown_context(self, context: str) -> dict:
        """Parse markdown context into structured data"""
        import re
        
        data = {
            'fees': [],
            'routes': [],
            'facilities': [],
            'programs': [],
            'general_info': []
        }
        
        lines = context.split('\n')
        current_item = {}
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # Extract fee information
            if 'bedoccupancy:' in line.lower():
                beds = re.search(r'(\d+)', line)
                if beds:
                    current_item['beds'] = beds.group(1)
            
            if 'roomtype:' in line.lower():
                room_type = line.split(':', 1)[1].strip().replace('**', '').strip()
                current_item['room_type'] = room_type
            
            if any(x in line.lower() for x in ['ladieshostel:', 'menshostel:']):
                price = re.search(r'(\d+)', line)
                if price and 'nil' not in line.lower():
                    current_item['price'] = price.group(1)
                    
                    # Complete the fee item
                    if 'beds' in current_item and 'room_type' in current_item:
                        data['fees'].append(current_item.copy())
                        current_item = {}
            
            # Extract route information
            if 'destination:' in line.lower() or 'route:' in line.lower():
                route_info = line.split(':', 1)[1].strip().replace('**', '').strip()
                if route_info and len(route_info) > 2:
                    data['routes'].append(route_info)
            
            # Extract general information
            if line.startswith('- **') and ':' in line:
                key_value = line.replace('- **', '').replace('**', '')
                if ':' in key_value:
                    key, value = key_value.split(':', 1)
                    key = key.strip().lower()
                    value = value.strip()
                    
                    if key not in ['category', 'lastupdated', 'academicyear'] and value:
                        data['general_info'].append(f"{key.title()}: {value}")
        
        return data
    
    def _generate_llm_style_response(self, query: str, query_lower: str, data: dict, full_context: str) -> str:
        """Generate natural language response like an LLM would"""
        
        # Hostel fee queries
        if 'hostel' in query_lower and any(word in query_lower for word in ['fee', 'cost', 'price', 'charge']):
            if data['fees']:
                response = "Here are the hostel fees for 2025-2026 academic year:\n\n"
                
                # Group by bed occupancy
                for fee in data['fees']:
                    beds = fee.get('beds', '')
                    room_type = fee.get('room_type', '')
                    price = fee.get('price', '')
                    
                    if beds and room_type and price:
                        response += f"â€¢ {beds}-bed sharing ({room_type}): â‚¹{price:,} per year\n"
                
                response += "\nðŸ’¡ Note: Mess fees are included in the hostel fees."
                return response
        
        # Transport queries
        if any(word in query_lower for word in ['bus', 'transport', 'route', 'fare']):
            if data['routes']:
                response = "Here are the available bus routes:\n\n"
                for route in data['routes'][:10]:
                    response += f"â€¢ {route}\n"
                response += "\nFor detailed timings and fares, please contact the transport office."
                return response
        
        # Program queries
        if any(word in query_lower for word in ['program', 'course', 'degree', 'btech', 'mtech']):
            programs = []
            for line in full_context.split('\n'):
                if any(word in line.lower() for word in ['btech', 'mtech', 'engineering', 'program', 'degree']):
                    cleaned = line.replace('**', '').replace('- ', '').replace('#', '').strip()
                    if cleaned and 5 < len(cleaned) < 100:
                        programs.append(cleaned)
            
            if programs:
                response = "KARE offers the following programs:\n\n"
                for prog in programs[:15]:
                    response += f"â€¢ {prog}\n"
                return response
        
        # General hostel info
        if 'hostel' in query_lower:
            if data['general_info']:
                response = "Hostel Information:\n\n"
                for info in data['general_info'][:10]:
                    if any(word in info.lower() for word in ['hostel', 'mens', 'ladies', 'separate']):
                        response += f"â€¢ {info}\n"
                return response
        
        # Default: Show relevant general information
        if data['general_info']:
            response = "Here's what I found:\n\n"
            for info in data['general_info'][:8]:
                response += f"â€¢ {info}\n"
            return response
        
        # Final fallback
        return "I found relevant information in the knowledge base. Please try asking more specifically about admissions, fees, hostels, placements, programs, or facilities."
    
    def _clean_text(self, text: str) -> str:
        """Clean and format text for readability"""
        # Remove excessive whitespace
        text = ' '.join(text.split())
        
        # Format key-value pairs nicely
        if '|' in text:
            parts = text.split('|')
            formatted = []
            for part in parts:
                part = part.strip()
                if ':' in part:
                    key, val = part.split(':', 1)
                    formatted.append(f"â€¢ {key.strip()}: {val.strip()}")
                else:
                    formatted.append(f"â€¢ {part}")
            return '\n'.join(formatted)
        
        return text
    
    def _is_greeting(self, query: str) -> bool:
        """Check if query is a greeting"""
        greetings = [
            'hello', 'hi', 'hey', 'namaste', 'vanakkam',
            'how are you', 'how do you do', 'whats up'
        ]
        query_lower = query.lower().strip()
        
        # Very short greeting-like queries
        if len(query_lower) < 20:
            for greeting in greetings:
                if greeting in query_lower:
                    # Make sure it's not about university topics
                    uni_keywords = ['hostel', 'fee', 'admission', 'course', 'placement']
                    if not any(kw in query_lower for kw in uni_keywords):
                        return True
        
        return False
    
    def _get_greeting_response(self, language: str) -> str:
        """Get greeting response"""
        responses = {
            'en': "Hello! I'm KARE AI, your intelligent assistant for Kalasalingam Academy of Research and Education. I can help you with information about admissions, programs, fees, hostels, placements, facilities, and more. What would you like to know?",
            
            'hi': "à¤¨à¤®à¤¸à¥à¤¤à¥‡! à¤®à¥ˆà¤‚ KARE AI à¤¹à¥‚à¤, à¤•à¤²à¤¾à¤¸à¤²à¤¿à¤‚à¤—à¤® à¤à¤•à¥‡à¤¡à¤®à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤ªà¤•à¤¾ à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¾à¤¨ à¤¸à¤¹à¤¾à¤¯à¤•à¥¤ à¤®à¥ˆà¤‚ à¤ªà¥à¤°à¤µà¥‡à¤¶, à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®, à¤«à¥€à¤¸, à¤¹à¥‰à¤¸à¥à¤Ÿà¤², à¤ªà¥à¤²à¥‡à¤¸à¤®à¥‡à¤‚à¤Ÿ, à¤¸à¥à¤µà¤¿à¤§à¤¾à¤“à¤‚ à¤”à¤° à¤…à¤§à¤¿à¤• à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤à¥¤ à¤†à¤ª à¤•à¥à¤¯à¤¾ à¤œà¤¾à¤¨à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚?",
            
            'te': "à°¨à°®à°¸à±à°•à°¾à°°à°‚! à°¨à±‡à°¨à± KARE AI, à°•à°²à°¾à°¸à°²à°¿à°‚à°—à°‚ à°Žà°•à°¡à°®à±€ à°•à±Šà°°à°•à± à°®à±€ à°¤à±†à°²à°¿à°µà±ˆà°¨ à°¸à°¹à°¾à°¯à°•à±à°¡à°¨à±. à°¨à±‡à°¨à± à°ªà±à°°à°µà±‡à°¶à°¾à°²à±, à°•à°¾à°°à±à°¯à°•à±à°°à°®à°¾à°²à±, à°«à±€à°œà±, à°¹à°¾à°¸à±à°Ÿà±†à°²à±, à°ªà±à°²à±‡à°¸à±â€Œà°®à±†à°‚à°Ÿà±à°¸à±, à°¸à±Œà°•à°°à±à°¯à°¾à°²à± à°®à°°à°¿à°¯à± à°®à°°à°¿à°¨à±à°¨à°¿ à°—à±à°°à°¿à°‚à°šà°¿ à°¸à°®à°¾à°šà°¾à°°à°‚à°¤à±‹ à°¸à°¹à°¾à°¯à°‚ à°šà±‡à°¯à°—à°²à°¨à±. à°®à±€à°°à± à°à°®à°¿ à°¤à±†à°²à±à°¸à±à°•à±‹à°µà°¾à°²à°¨à±à°•à±à°‚à°Ÿà±à°¨à±à°¨à°¾à°°à±?",
            
            'ta': "à®µà®£à®•à¯à®•à®®à¯! à®¨à®¾à®©à¯ KARE AI, à®•à®²à®¾à®šà®²à®¿à®™à¯à®•à®®à¯ à®…à®•à®¾à®Ÿà®®à®¿à®•à¯à®•à®¾à®© à®‰à®™à¯à®•à®³à¯ à®…à®±à®¿à®µà®¾à®°à¯à®¨à¯à®¤ à®‰à®¤à®µà®¿à®¯à®¾à®³à®°à¯. à®šà¯‡à®°à¯à®•à¯à®•à¯ˆ, à®¤à®¿à®Ÿà¯à®Ÿà®™à¯à®•à®³à¯, à®•à®Ÿà¯à®Ÿà®£à®®à¯, à®µà®¿à®Ÿà¯à®¤à®¿, à®µà¯‡à®²à¯ˆà®µà®¾à®¯à¯à®ªà¯à®ªà¯, à®µà®šà®¤à®¿à®•à®³à¯ à®®à®±à¯à®±à¯à®®à¯ à®ªà®²à®µà®±à¯à®±à¯ˆà®ªà¯ à®ªà®±à¯à®±à®¿à®¯ à®¤à®•à®µà®²à¯à®•à®³à®¿à®²à¯ à®¨à®¾à®©à¯ à®‰à®™à¯à®•à®³à¯à®•à¯à®•à¯ à®‰à®¤à®µ à®®à¯à®Ÿà®¿à®¯à¯à®®à¯. à®¨à¯€à®™à¯à®•à®³à¯ à®Žà®©à¯à®© à®¤à¯†à®°à®¿à®¨à¯à®¤à¯à®•à¯Šà®³à¯à®³ à®µà®¿à®°à¯à®®à¯à®ªà¯à®•à®¿à®±à¯€à®°à¯à®•à®³à¯?",
            
            'kn': "à²¨à²®à²¸à³à²•à²¾à²°! à²¨à²¾à²¨à³ KARE AI, à²•à²²à²¾à²¸à²²à²¿à²‚à²—à²®à³ à²…à²•à²¾à²¡à³†à²®à²¿à²—à²¾à²—à²¿ à²¨à²¿à²®à³à²® à²¬à³à²¦à³à²§à²¿à²µà²‚à²¤ à²¸à²¹à²¾à²¯à²•. à²ªà³à²°à²µà³‡à²¶à²—à²³à³, à²•à²¾à²°à³à²¯à²•à³à²°à²®à²—à²³à³, à²¶à³à²²à³à²•, à²µà²¸à²¤à²¿, à²¨à²¿à²¯à³‹à²œà²¨à³†à²—à²³à³, à²¸à³Œà²•à²°à³à²¯à²—à²³à³ à²®à²¤à³à²¤à³ à²¹à³†à²šà³à²šà²¿à²¨à²¦à²° à²¬à²—à³à²—à³† à²®à²¾à²¹à²¿à²¤à²¿à²¯à³Šà²‚à²¦à²¿à²—à³† à²¨à²¾à²¨à³ à²¨à²¿à²®à²—à³† à²¸à²¹à²¾à²¯ à²®à²¾à²¡à²¬à²²à³à²²à³†. à²¨à³€à²µà³ à²à²¨à³ à²¤à²¿à²³à²¿à²¯à²²à³ à²¬à²¯à²¸à³à²¤à³à²¤à³€à²°à²¿?",
            
            'ml': "à´¨à´®à´¸àµà´•à´¾à´°à´‚! à´žà´¾àµ» KARE AI, à´•à´²à´¾à´¸à´²à´¿à´‚à´—à´‚ à´…à´•àµà´•à´¾à´¦à´®à´¿à´•àµà´•àµ à´µàµ‡à´£àµà´Ÿà´¿à´¯àµà´³àµà´³ à´¨à´¿à´™àµà´™à´³àµà´Ÿàµ† à´¬àµà´¦àµà´§à´¿à´ªà´°à´®à´¾à´¯ à´¸à´¹à´¾à´¯à´¿. à´ªàµà´°à´µàµ‡à´¶à´¨à´‚, à´ªàµà´°àµ‹à´—àµà´°à´¾à´®àµà´•àµ¾, à´«àµ€à´¸àµ, à´¹àµ‹à´¸àµà´±àµà´±àµ½, à´ªàµà´²àµ†à´¯àµà´¸àµà´®àµ†à´¨àµà´±àµà´•àµ¾, à´¸àµ—à´•à´°àµà´¯à´™àµà´™àµ¾ à´Žà´¨àµà´¨à´¿à´µà´¯àµ†à´•àµà´•àµà´±à´¿à´šàµà´šàµà´³àµà´³ à´µà´¿à´µà´°à´™àµà´™à´³à´¿àµ½ à´žà´¾àµ» à´¨à´¿à´™àµà´™à´³àµ† à´¸à´¹à´¾à´¯à´¿à´•àµà´•à´¾àµ» à´•à´´à´¿à´¯àµà´‚. à´¨à´¿à´™àµà´™àµ¾ à´Žà´¨àµà´¤à´¾à´£àµ à´…à´±à´¿à´¯à´¾àµ» à´†à´—àµà´°à´¹à´¿à´•àµà´•àµà´¨àµà´¨à´¤àµ?",
        }
        
        return responses.get(language, responses['en'])
    
    def _get_no_info_response(self, query: str, language: str) -> str:
        """Response when no information found"""
        responses = {
            'en': f"I don't have specific information about '{query}'. Please try asking about admissions, fees, hostels, placements, programs, facilities, or contact information.",
            
            'hi': f"à¤®à¥à¤à¥‡ '{query}' à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆà¥¤ à¤•à¥ƒà¤ªà¤¯à¤¾ à¤ªà¥à¤°à¤µà¥‡à¤¶, à¤«à¥€à¤¸, à¤¹à¥‰à¤¸à¥à¤Ÿà¤², à¤ªà¥à¤²à¥‡à¤¸à¤®à¥‡à¤‚à¤Ÿ, à¤•à¤¾à¤°à¥à¤¯à¤•à¥à¤°à¤®, à¤¸à¥à¤µà¤¿à¤§à¤¾à¤à¤‚ à¤¯à¤¾ à¤¸à¤‚à¤ªà¤°à¥à¤• à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤ªà¥‚à¤›à¥‡à¤‚à¥¤",
            
            'te': f"à°¨à°¾à°•à± '{query}' à°—à±à°°à°¿à°‚à°šà°¿ à°¨à°¿à°°à±à°¦à°¿à°·à±à°Ÿ à°¸à°®à°¾à°šà°¾à°°à°‚ à°²à±‡à°¦à±. à°¦à°¯à°šà±‡à°¸à°¿ à°ªà±à°°à°µà±‡à°¶à°¾à°²à±, à°«à±€à°œà±, à°¹à°¾à°¸à±à°Ÿà±†à°²à±, à°ªà±à°²à±‡à°¸à±â€Œà°®à±†à°‚à°Ÿà±à°¸à±, à°•à°¾à°°à±à°¯à°•à±à°°à°®à°¾à°²à±, à°¸à±Œà°•à°°à±à°¯à°¾à°²à± à°²à±‡à°¦à°¾ à°¸à°‚à°ªà±à°°à°¦à°¿à°‚à°ªà± à°¸à°®à°¾à°šà°¾à°°à°‚ à°—à±à°°à°¿à°‚à°šà°¿ à°…à°¡à°—à°‚à°¡à°¿à¥¤",
            
            'ta': f"à®Žà®©à®•à¯à®•à¯ '{query}' à®ªà®±à¯à®±à®¿à®¯ à®•à¯à®±à®¿à®ªà¯à®ªà®¿à®Ÿà¯à®Ÿ à®¤à®•à®µà®²à¯ à®‡à®²à¯à®²à¯ˆ. à®šà¯‡à®°à¯à®•à¯à®•à¯ˆ, à®•à®Ÿà¯à®Ÿà®£à®®à¯, à®µà®¿à®Ÿà¯à®¤à®¿, à®µà¯‡à®²à¯ˆà®µà®¾à®¯à¯à®ªà¯à®ªà¯, à®¤à®¿à®Ÿà¯à®Ÿà®™à¯à®•à®³à¯, à®µà®šà®¤à®¿à®•à®³à¯ à®…à®²à¯à®²à®¤à¯ à®¤à¯Šà®Ÿà®°à¯à®ªà¯ à®¤à®•à®µà®²à¯ à®ªà®±à¯à®±à®¿ à®•à¯‡à®Ÿà¯à®•à®µà¯à®®à¯.",
            
            'kn': f"à²¨à²¨à²—à³† '{query}' à²¬à²—à³à²—à³† à²¨à²¿à²°à³à²¦à²¿à²·à³à²Ÿ à²®à²¾à²¹à²¿à²¤à²¿ à²‡à²²à³à²². à²¦à²¯à²µà²¿à²Ÿà³à²Ÿà³ à²ªà³à²°à²µà³‡à²¶à²—à²³à³, à²¶à³à²²à³à²•, à²µà²¸à²¤à²¿, à²¨à²¿à²¯à³‹à²œà²¨à³†à²—à²³à³, à²•à²¾à²°à³à²¯à²•à³à²°à²®à²—à²³à³, à²¸à³Œà²•à²°à³à²¯à²—à²³à³ à²…à²¥à²µà²¾ à²¸à²‚à²ªà²°à³à²• à²®à²¾à²¹à²¿à²¤à²¿à²¯ à²¬à²—à³à²—à³† à²•à³‡à²³à²¿.",
            
            'ml': f"à´Žà´¨à´¿à´•àµà´•àµ '{query}' à´¸à´‚à´¬à´¨àµà´§à´¿à´šàµà´šàµ à´¨à´¿àµ¼à´¦àµà´¦à´¿à´·àµà´Ÿ à´µà´¿à´µà´°à´™àµà´™àµ¾ à´‡à´²àµà´². à´¦à´¯à´µà´¾à´¯à´¿ à´ªàµà´°à´µàµ‡à´¶à´¨à´‚, à´«àµ€à´¸àµ, à´¹àµ‹à´¸àµà´±àµà´±àµ½, à´ªàµà´²àµ†à´¯àµà´¸àµà´®àµ†à´¨àµà´±àµà´•àµ¾, à´ªàµà´°àµ‹à´—àµà´°à´¾à´®àµà´•àµ¾, à´¸àµ—à´•à´°àµà´¯à´™àµà´™àµ¾ à´…à´²àµà´²àµ†à´™àµà´•à´¿àµ½ à´¬à´¨àµà´§à´ªàµà´ªàµ†à´Ÿàµ½ à´µà´¿à´µà´°à´™àµà´™à´³àµ†à´•àµà´•àµà´±à´¿à´šàµà´šàµ à´šàµ‹à´¦à´¿à´•àµà´•àµà´•.",
        }
        
        return responses.get(language, responses['en'])
    
    def _get_acknowledgment(self, language: str) -> str:
        """Get acknowledgment message"""
        ack = {
            'en': "Here's what I found:",
            'hi': "à¤¯à¤¹à¤¾à¤ à¤®à¥à¤à¥‡ à¤•à¥à¤¯à¤¾ à¤®à¤¿à¤²à¤¾:",
            'te': "à°‡à°•à±à°•à°¡ à°¨à±‡à°¨à± à°•à°¨à±à°—à±Šà°¨à±à°¨à°µà°¿:",
            'ta': "à®¨à®¾à®©à¯ à®•à®£à¯à®Ÿà®¤à¯ à®‡à®¤à¯‹:",
            'kn': "à²¨à²¾à²¨à³ à²•à²‚à²¡à³à²•à³Šà²‚à²¡à²¦à³à²¦à³ à²‡à²²à³à²²à²¿à²¦à³†:",
            'ml': "à´žà´¾àµ» à´•à´£àµà´Ÿàµ†à´¤àµà´¤à´¿à´¯à´¤àµ à´‡à´¤à´¾à´£àµ:",
        }
        return ack.get(language, ack['en'])
    
    def _get_closing(self, language: str) -> str:
        """Get closing message"""
        closing = {
            'en': "Is there anything else you'd like to know?",
            'hi': "à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤”à¤° à¤•à¥à¤› à¤œà¤¾à¤¨à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚?",
            'te': "à°®à±€à°°à± à°‡à°‚à°•à°¾ à°à°¦à±ˆà°¨à°¾ à°¤à±†à°²à±à°¸à±à°•à±‹à°µà°¾à°²à°¨à±à°•à±à°‚à°Ÿà±à°¨à±à°¨à°¾à°°à°¾?",
            'ta': "à®µà¯‡à®±à¯ à®à®¤à®¾à®µà®¤à¯ à®¤à¯†à®°à®¿à®¨à¯à®¤à¯ à®•à¯Šà®³à¯à®³ à®µà®¿à®°à¯à®®à¯à®ªà¯à®•à®¿à®±à¯€à®°à¯à®•à®³à®¾?",
            'kn': "à²¨à³€à²µà³ à²‡à²¨à³à²¨à³‡à²¨à²¾à²¦à²°à³‚ à²¤à²¿à²³à²¿à²¯à²²à³ à²¬à²¯à²¸à³à²¤à³à²¤à³€à²°à²¾?",
            'ml': "à´¨à´¿à´™àµà´™àµ¾à´•àµà´•àµ à´®à´±àµà´±àµ†à´¨àµà´¤àµ†à´™àµà´•à´¿à´²àµà´‚ à´…à´±à´¿à´¯à´£àµ‹?",
        }
        return closing.get(language, closing['en'])
    
    def get_system_info(self) -> Dict:
        """Get information about RAG system"""
        return {
            "vector_store_loaded": self.vector_store.index is not None,
            "total_chunks": len(self.vector_store.chunks) if self.vector_store.chunks else 0,
            "llm_available": self.llm_available,
            "llm_model": self.llm.get_model_info() if self.llm else None,
        }


# Global instance
_rag_generator = None

def get_rag_generator(
    model_path: Optional[str] = None,
    use_llm: bool = False,  # Default to False until model is downloaded
    **kwargs
) -> RAGResponseGenerator:
    """Get or create global RAG generator instance"""
    global _rag_generator
    
    if _rag_generator is None:
        _rag_generator = RAGResponseGenerator(
            model_path=model_path,
            use_llm=use_llm,
            **kwargs
        )
    
    return _rag_generator


def generate_rag_response(query: str, language: str = 'en', top_k: int = 3) -> str:
    """Utility function for generating RAG responses"""
    generator = get_rag_generator()
    return generator.generate_response(query, language, top_k)


if __name__ == "__main__":
    # Test RAG system
    logging.basicConfig(level=logging.INFO)
    
    print("ðŸ§ª Testing RAG Response Generator")
    print("=" * 70)
    
    # Initialize (without LLM for now)
    rag = RAGResponseGenerator(use_llm=False)
    
    print(f"\nðŸ“Š System Info:")
    info = rag.get_system_info()
    print(f"   Vector Store Loaded: {info['vector_store_loaded']}")
    print(f"   Total Chunks: {info['total_chunks']}")
    print(f"   LLM Available: {info['llm_available']}")
    
    # Test queries
    test_queries = [
        ("What is the hostel fee?", "en"),
        ("Tell me about bus routes", "en"),
        ("admission process kya hai?", "hi"),
        ("programs gurinchi cheppandi", "te"),
    ]
    
    for query, lang in test_queries:
        print(f"\n{'='*70}")
        print(f"Query: {query} (Language: {lang})")
        print(f"{'='*70}")
        
        response = rag.generate_response(query, language=lang, top_k=2)
        print(f"\nResponse:\n{response}")
